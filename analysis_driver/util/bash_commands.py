import os.path
from egcg_core.app_logging import logging_default as log_cfg
from analysis_driver.config import default as cfg
from analysis_driver.tool_versioning import toolset
from analysis_driver.exceptions import AnalysisDriverError


app_logger = log_cfg.get_logger(__name__)


def bcl2fastq_per_lane(input_dir, fastq_dir, sample_sheet, masks, lanes):
    cmds = []
    for mask, lane in zip(masks, lanes):
        lane_fastq_dir = os.path.join(fastq_dir, 'lane_' + str(lane))
        cmds.append(bcl2fastq(input_dir, lane_fastq_dir, sample_sheet, mask, lane))
    return cmds


def bcl2fastq(input_dir, fastq_path, sample_sheet=None, mask=None, lane=None):
    """
    Build a bcl2fastq command for an Illumina HiSeqX dataset.
    :param input_dir: Path to the input dir containing the bcl files
    :param fastq_path: Path to the dir in which to send generated .fastqs
    :param str sample_sheet: Path to a bcl2fast2 sample sheet to use
    :param str mask: A mask to use, as generated by reader.RunInfo
    :param int lane: The number of the lane that should be processed
    """
    cmd = '%s -l INFO --runfolder-dir %s --output-dir %s -r 8 -p 8 -w 8' % (
        toolset['bcl2fastq'], input_dir, fastq_path
    )
    if sample_sheet:
        cmd += ' --sample-sheet ' + sample_sheet
    if mask:
        cmd += ' --use-bases-mask ' + mask
    if lane:
        cmd += ' --tiles s_' + str(lane)
    app_logger.debug('Writing: ' + cmd)
    return cmd


def fastqc(fastq, threads=1, tmp_dir=None):
    if not tmp_dir:
        tmp_dir = os.getcwd()
    cmd = toolset['fastqc'] + ' --nogroup -t %s --dir %s -q %s' % (threads, tmp_dir, fastq)
    app_logger.debug('Writing: ' + cmd)
    return cmd


def gzip_test(f):
    cmd = toolset['gzip'] + ' -t ' + f
    app_logger.debug('Writing: ' + cmd)
    return cmd


def seqtk_fqchk(fastq_file):
    cmd = toolset['seqtk'] + ' fqchk -q 0 %s > %s.fqchk' % (fastq_file, fastq_file)
    app_logger.debug('Writing: ' + cmd)
    return cmd


def fq_filt_prelim_cmd():
    cmd = (
        'function run_filterer {{',  # double up { and } to escape them for str.format()
        'strategy=$1', 'i1=$2', 'i2=$3', 'o1=$4', 'o2=$5', 'fifo_1=$6', 'fifo_2=$7', 'f1=$8', 'f2=$9',
        'shift 9',
        'mkfifo $fifo_1', 'mkfifo $fifo_2',
        '{ff} --i1 $i1 --i2 $i2 --o1 $fifo_1 --o2 $fifo_2 --f1 $f1 --f2 $f2 $* &',
        'fq_filt_pid=$!',
        '{pigz} -c -p {pzt} $fifo_1 > $o1 &',
        'pigz_r1_pid=$!',
        '{pigz} -c -p {pzt} $fifo_2 > $o2 &',
        'pigz_r2_pid=$!',

        'exit_status=0',
        'wait $fq_filt_pid',
        'exit_status=$[$exit_status + $?]',
        'wait $pigz_r1_pid',
        'exit_status=$[$exit_status + $?]',
        'wait $pigz_r2_pid',
        'exit_status=$[$exit_status + $?]',

        '{pigz} -p {pzt} $out_phix1 &',
        'pigz_phix1_pid=$!',
        '{pigz} -p {pzt} $out_phix2 &',
        'pigz_phix2_pid=$!',

        'wait $pigz_phix1_pid',
        'exit_status=$[$exit_status + $?]',
        'wait $pigz_phix2_pid',
        'exit_status=$[$exit_status + $?]',

        'rm $fifo_1 $fifo_2',
        'if [ $exit_status == 0 ]; then',
        'if [ "$strategy" == "keep_originals" ]; then mv $i1 $i1.original; mv $i2 $i2.original; fi',
        'mv $o1 $i1; mv $o2 $i2',
        'fi',
        '(exit $exit_status)',
        '}}'
    )
    return '\n'.join(cmd).format(
        ff=toolset['fastq_filterer'],
        pigz=toolset['pigz'],
        pzt=10  # two pigz processes run, so pigz threads here will be doubled
    )


def fastq_filterer(fastq_file_pair, threshold=None, rm_tiles=None, rm_reads=None, trim_r1=None, trim_r2=None,
                   quiet=False, unsafe=False, stats_file=None):
    """
    :param tuple[str,str] fastq_file_pair: Paired-end fastqs to filter
    :param int threshold: Read length to filter on - defaults to config or 36 bases
    :param list rm_tiles: Tile IDs for reads to remove regardless of length
    :param str rm_reads: File containing read names to pass to --remove_reads
    :param trim_r1: Maximum length to trim R1 to
    :param trim_r2: As trim_r1, but for R2
    :param bool quiet: Pass --quiet to filterer
    :param bool unsafe: Pass --unsafe to filterer
    :param str stats_file: Alternate name for written stats file - defaults to fastq_basename_fastqfilterer.stats
    Run fastq filterer on pairs of fastqs, removing read pairs where one is shorter than 36 bases
    """

    if len(fastq_file_pair) != 2:
        raise AnalysisDriverError('fastq-filterer only supports paired fastq files')

    i1, i2 = sorted(fastq_file_pair)

    base_1 = i1.replace('.fastq.gz', '')
    f1 = base_1 + '.fastq_discarded'
    fifo_1 = base_1 + '_filtered.fastq'
    o1 = fifo_1 + '.gz'

    base_2 = i2.replace('.fastq.gz', '')
    f2 = base_2 + '.fastq_discarded'  # add suffix to avoid collision with find_fastq functions
    fifo_2 = base_2 + '_filtered.fastq'
    o2 = fifo_2 + '.gz'

    if any((rm_tiles, trim_r1, trim_r2)):
        strategy = 'keep_originals'
    else:
        strategy = 'in_place'

    # args passed to fq_file_prelim_cmd bash function
    cmd = 'run_filterer {0} {1} {2} {3} {4} {5} {6} {7} {8}'.format(
        strategy, i1, i2, o1, o2, fifo_1, fifo_2, f1, f2
    )

    # all other args passed as normal
    threshold = threshold or cfg.query('fastq_filterer', 'min_length', ret_default='36')
    stats_file = stats_file or base_1.replace('_R1_001', '') + '_fastqfilterer.stats'

    cmd += ' --threshold %s' % threshold
    cmd += ' --stats_file %s' % stats_file

    if rm_tiles:
        cmd += ' --remove_tiles %s' % (','.join(str(t) for t in rm_tiles))
    if rm_reads:
        cmd += ' --remove_reads %s' % rm_reads
    if trim_r1:
        cmd += ' --trim_r1 %s' % trim_r1
    if trim_r2:
        cmd += ' --trim_r2 %s' % trim_r2
    if quiet:
        cmd += ' --quiet'
    if unsafe:
        cmd += ' --unsafe'

    app_logger.debug('Writing: ' + cmd)
    return cmd


def bwa_mem_samblaster(fastq_pair, reference, expected_output_bam, read_group=None, thread=16):
    tmp_dir = os.path.dirname(expected_output_bam)
    command_bwa = '%s mem -M -t %s' % (toolset['bwa'], thread)

    if read_group:
        read_group_str = '@RG\\t%s' % '\\t'.join(['%s:%s' % (k, read_group[k]) for k in sorted(read_group)])
        command_bwa += ' -R \'%s\'' % read_group_str

    command_bwa += ' %s %s' % (reference, ' '.join(fastq_pair))
    command_samtools = toolset['samtools'] + ' view -b -'
    command_sambamba = '%s sort -m 5G --tmpdir %s -t %s -o %s /dev/stdin' % (
        toolset['sambamba'], tmp_dir, thread, expected_output_bam
    )
    cmd = 'set -o pipefail; ' + ' | '.join([command_bwa, toolset['samblaster'], command_samtools, command_sambamba])
    app_logger.debug('Writing: ' + cmd)
    return cmd


def bwa_mem_phix(fastq, read_name_list, fasta, thread=16):
    command_bwa = \
        '%s mem -t %s %s %s' % (toolset['bwa'], thread, fasta, fastq)
    command_samtools = '%s view -F 4 | cut -f 1 | sort -u > %s' % (toolset['samtools'], read_name_list)
    cmd = 'set -o pipefail; ' + ' | '.join([command_bwa, command_samtools])
    app_logger.debug('Writing: ' + cmd)
    return cmd


def bwa_mem_biobambam(fastq_pair, reference, expected_output_bam, read_group=None, thread=16):
    tmp_file = expected_output_bam
    index = expected_output_bam + '.bai'
    command_bwa = '%s mem -M -t %s' % (toolset['bwa'], thread)

    if read_group:
        read_group_str = '@RG\\t%s' % '\\t'.join(['%s:%s' % (k, read_group[k]) for k in sorted(read_group)])
        command_bwa += ' -R \'%s\'' % read_group_str

    command_bwa += ' %s %s' % (reference, ' '.join(fastq_pair))
    command_bambam = '%s inputformat=sam SO=coordinate tmpfile=%s threads=%s indexfilename=%s > %s' % (
        toolset['biobambam_sortmapdup'], tmp_file, thread, index, expected_output_bam
    )

    cmd = 'set -o pipefail; ' + ' | '.join([command_bwa, command_bambam])
    app_logger.debug('Writing: ' + cmd)
    return cmd


def samtools_stats(bam_file, output_file):
    cmd = '%s stats %s > %s' % (toolset['samtools'], bam_file, output_file)
    app_logger.debug('Writing: ' + cmd)
    return cmd


def samtools_depth_command(job_dir, bam_file, out_file):
    cmd = '%s depth -a -a -q 0 -Q 0 %s | '\
          'awk -F "\t" \'{array[$1"\t"$3]+=1} END{for (val in array){print val"\t"array[val]}}\' | '\
          'sort -T %s -k 1,1 -nk 2,2 > %s' % (toolset['samtools'], bam_file, job_dir, out_file)
    app_logger.debug('Writing: ' + cmd)
    return cmd


def md5sum(input_file):
    cmd = toolset['md5sum'] + ' %s > %s.md5' % (input_file, input_file)
    app_logger.debug('Writing: ' + cmd)
    return cmd


def picard_command(program, input_file, output_file, tmp_dir, memory, assume_sorted=True, picard_params=None):
    cmd = (java_command(memory, tmp_dir or os.path.dirname(input_file), toolset['picard']) + '{program} '
           'INPUT={input} OUTPUT={output} VALIDATION_STRINGENCY=LENIENT'
           )
    if picard_params is None:
        picard_params = {}
    if assume_sorted:
        picard_params['ASSUME_SORTED'] = 'true'
    if picard_params:
        for k in sorted(picard_params):
            cmd += ' %s=%s' % (k, picard_params[k])

    cmd = cmd.format(input=input_file, output=output_file, program=program)

    app_logger.debug('Writing: ' + cmd)
    return cmd


def picard_gc_bias(input_file, metrics, summary, chart, ref, memory=8, tmp_dir=None):
    return picard_command(
        'CollectGcBiasMetrics', input_file, metrics, tmp_dir, memory,
        picard_params={'SUMMARY_OUTPUT': summary, 'CHART': chart, 'R': ref}
    )


def picard_mark_dup_command(input_file, output_file, metrics_file, memory=10, tmp_dir=None):
    return picard_command(
        'MarkDuplicates', input_file, output_file, tmp_dir, memory,
        picard_params={'METRICS_FILE': metrics_file, 'OPTICAL_DUPLICATE_PIXEL_DISTANCE': '100'}
    )


def picard_insert_size_command(input_file, metrics_file, histogram_file, memory=8, tmp_dir=None):
    return picard_command(
        'CollectInsertSizeMetrics', input_file, metrics_file, tmp_dir, memory,
        picard_params={'HISTOGRAM_FILE': histogram_file}
    )


def export_env_vars():
    """Write export statements for environment variables required by BCBio"""
    app_logger.debug('Setting custom library paths')
    return (
        'export PATH=%s:%s:$PATH' % (
            os.path.join(toolset['bcbio'], 'bin'), os.path.join(toolset['jdk'], 'bin')
        ),
        'export LD_LIBRARY_PATH=%s:$LD_LIBRARY_PATH' % os.path.join(toolset['bcbio'], 'lib'),
        'export PERL5LIB=%s:$PERL5LIB' % os.path.join(toolset['bcbio'], 'lib', 'perl5'),
        'export JAVA_HOME=' + toolset['jdk'],
        'export JAVA_BINDIR=' + os.path.join(toolset['jdk'], 'bin'),
        'export JAVA_ROOT=' + toolset['jdk'],
        ''
    )


def bcbio(run_yaml, workdir, threads=10):
    cmd = '%s %s -n %s --workdir %s' % (
        os.path.join(toolset['bcbio'], 'bin', 'bcbio_nextgen.py'), run_yaml, threads, workdir
    )
    app_logger.debug('Writing: ' + cmd)
    return cmd


def bcbio_prepare_samples(job_dir, bcbio_csv_file):
    """
    Call bcbio_prepare_samples with a csv sample file and a list of fastqs.
    :param str job_dir: Full path to the job folder
    :param str bcbio_csv_file: Full path to the BCBio csv
    """
    merged_dir = os.path.join(job_dir, 'merged')
    cmd = '{bcbio} --out {d} --csv {csv}'.format(
        bcbio=os.path.join(toolset['bcbio'], 'bin', 'bcbio_prepare_samples.py'),
        d=merged_dir, csv=bcbio_csv_file
    )
    app_logger.debug('Writing: ' + cmd)
    return cmd


def is_remote_path(fp):
    return (':' in fp) and ('@' in fp)


def rsync_from_to(source, dest, exclude=None, size_only=False):
    command = 'rsync -rLD '
    if exclude:
        command += '--exclude=%s ' % exclude
    if size_only:
        command += '--size-only '
    else:
        command += '--update '
    if is_remote_path(source) or is_remote_path(dest):
        command += '-e "ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -c arcfour" '

    command += '%s %s' % (source, dest)
    app_logger.debug('Writing: ' + command)
    return command


def java_command(memory, tmp_dir, jar):
    cmd = '{java} -Djava.io.tmpdir={tmp_dir} -XX:+UseSerialGC -Xmx{memory}G -jar {jar} '.format(
        java=toolset['java'],
        memory=memory,
        tmp_dir=tmp_dir,
        jar=jar
    )
    app_logger.debug('Writing: ' + cmd)
    return cmd


def bgzip_command(input_file):
    cmd = '%s -f %s' % (toolset['bgzip'], input_file)
    app_logger.debug('Writing: ' + cmd)
    return cmd


def tabix_vcf_command(input_file):
    cmd = '%s -f -p vcf %s' % (toolset['tabix'], input_file)
    app_logger.debug('Writing: ' + cmd)
    return cmd
