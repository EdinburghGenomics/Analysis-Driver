# Analysis Driver
[![travis](https://img.shields.io/travis/EdinburghGenomics/Analysis-Driver/master.svg)](https://travis-ci.org/EdinburghGenomics/Analysis-Driver)
[![landscape](https://landscape.io/github/EdinburghGenomics/Analysis-Driver/master/landscape.svg)](https://landscape.io/github/EdinburghGenomics/Analysis-Driver)
[![GitHub issues](https://img.shields.io/github/issues/EdinburghGenomics/Analysis-Driver.svg)](https://github.com/EdinburghGenomics/Analysis-Driver/issues)

(Note: this project uses EGCG-Core, which is available [here](https://github.com/EdinburghGenomics/EGCG-Core.git).)

## Scripts
These can be found in bin/. Currently, these are:

- edingen_analysis_driver.py - The main entry point for the pipeline, currently called by ProcTrigger with an input
  data dir.
- run_qc.py - Entry point for rerunning the QC for a given run or sample.
- send_data.py - Entry point for rerunning the QC crawlers for a given run/sample, pushing/re-pushing QC data
  to the [reporting app](https://github.com/EdinburghGenomics/Reporting-App).


## Modules
The Analysis Driver consists of several modules, each in turn consisting of several files/functions/classes:

### quality_control
Classes that run checks on output files generated from the main pipeline.

- GenotypeValidation - Uses bwa, samtools and gatk to validate called snps against a test dataset. Writes a
  vcf of genotyping results, queries the LIMS for an expected genotype vcf, and writes a file containing the
  results of comparing the observed and expected vcfs.
- GenderValidation - Quantifies X-chromosome heterozygosity in BCBio's output haplotype vcf. Produces a file
  containing the called gender, to be compared against the gender suppied in the Lims.
- ContaminationCheck - Checks fastqs for contamination using [fastqscreen](http://www.bioinformatics.babraham.ac.uk/projects/download.html#fastqscreen)
- ContaminationBlast - Checks a fastq file for contamination using NCBI Blast.
- VerifyBamID - Checks a Bam file for contamination using [VerifyBamID](http://genome.sph.umich.edu/wiki/VerifyBamID)
- VCFStats - Extracts VCF metrics using [rtg vcfstats](https://github.com/RealTimeGenomics/rtg-tools)
- SamtoolsDepth - Uses samtools to calculate median coverage in a Bam file.

### reader
Parsers for files supplied and generated during the pipeline.

#### run_info

- RunInfo - Represents an instance of RunInfo.xml. Reads in the xml and constructs a Mask object from the
  `<Reads>` element.
- Mask - Represents a read mask to be passed to bcl2fastq via sample_sheet.generate_mask.

#### sample_sheet

- transform_sample_sheet - The sample sheet automatically generated by the Lims is not entirely compatible
  with bcl2fastq, as some column names can vary. This function reads in SampleSheet.csv, uses
  `etc/sample_sheet_cfg.yaml` to modify column names, and outputs SampleSheet_analysis_driver.csv.

- SampleSheet - Reads SampleSheet_analysis_driver.csv and constructs a series of SampleProject objects. Can be
  constructed in a 'barcoded' or 'non-barcoded' mode. If non-barcoded, it checks that there is only one sample
  in each lane. Also uses `etc/sample_sheet_cfg.yaml` to allow for variable column names.
  - check_barcodes - For each project in the sample sheet, check that the barcode lengths all match.
  - generate_mask - Translates the `<Read>` elements in RunInfo.xml into a bcl2fastq mask.

- SampleProject - Represents a project ID in the sample sheet, which in turn will contain many samples.
- SampleID - Represents a single saple ID in the sample sheet.
- Sample - Represents a line in the sample sheet.

#### mapping_stats_parsers
Functions that pick up QC information from files generated by BCBio, including `bamtools_stats.txt`,
`sort-callable.bed`, `highdepth-stats.yaml` and fastqc reports.

#### demultiplexing_parsers
Functions for picking up QC information from `ConversionStats.xml` and `demultiplexing_stats.xml` (generated
by bcl2fastq) and also files produced by seqtk, containing base counts with adaptors removed.

### util
- bcbio_prepare_samples_cmd - Bash command to execute bcbio_prepare_samples.py for a set of input fastqs,
  merging them. Calls `_write_bcbio_csv`.
- write_bcbio_csv - Takes a sample id and a list of fastq files, and writes a csv to be passed to
  bcbio_prepare_samples.

#### bash_commands
Contains functions that build string Bash commands, including bcl2fastq, fastqc, seqtk, bwa mem/samblaster,
bamtools stats, md5sum, bcbio, rsync and export of environment variables.

### report_generation
Contains run and sample crawler classes that scan for QC files, parses them using functions from `reader` and
pushes data to an external Rest API data store.

### client
The main 'client' script for the Analysis Driver. Implements argparsing, sets up logging, adds handlers to the
logging configuration and subscribers to the notification centre, calls `dataset_scanner` to find new
datasets, and calls `process_trigger` on the first ready dataset. If the exit status is non-zero or if there
is a stack trace, the dataset will be marked as failed.

### config
Contains several config objects, which read various files in search paths and in `etc/`.

### dataset
When a pipeline is running, data about the pipeline run is handled by a `Dataset` object. The Dataset
interacts with the external Rest API to push and pull data, and also controls notifications.

- Dataset
  - most_recent_proc - An object representing the most recent process in the external Rest API, i.e. the
    record for the last time the dataset was processed by the pipeline (in case the dataset has been processed
    before)
  - dataset_status - Queries `self.most_recent_proc` to the get the running status of the dataset.
    `dataset_reprocess` is treated as invisible, and will show up as either `dataset_new` or `dataset_ready`.
  - running_stages - Returns the names of all currently running pipeline stages
  - start - Marks itself in the Rest API as running.
  - succeed, fail, abort, reset - Marks itself with the relevant status at the end of the pipeline run.
  - terminate - Gets the pid of a running pipeline and stops it. Optionally called in `client`.
    - is_valid_pid - Linux specific. Uses `/proc/<pid>/cmdline` to check that a pid is actually a running
      pipeline.
  - is_ready - `NotImplemented`

- NoCommunicationDataset - Fake Dataset with no Rest API interaction. Used in tests and utility scripts.

- RunDataset - Dataset specific to sequencing runs.
  - is_ready - True if RTAComplete.txt exists.

- SampleDataset - Dataset specific to samples. Handles the concept of 'run elements', where data for a sample
  can come from multiple sequencing runs.

  - run_elements - Gets all run elements that have been marked as useable.
  - non_useable_run_elements - Gets all run elements not marked as useable. Used in reporting.
  - amount_data - sums up the q30 bases from `self.run_elements` to get volume of useable data for the sample.
  - data_threshold - Queries the LIMS to get the sample's Yield for Quoted Coverage, i.e. the minimum volume
    of data required to run variant calling.
  - is_ready - True if `self.amount_data` > `self.data_threshold`.
  - force - Mark the sample for processing even if `self.is_ready` is False.

- ProjectDataset

- MostRecentProc - Represents the most up to date processing record in the Rest API. The unique identifier for
  these records is a concatenation of the dataset type, name, and start time.

  - entity - Cached copy of the process' Rest API data
  - retrieve_entity - Sets `self.entity` with data from the Rest API.
  - initialise_entity - Creates a new processing record, pushes it to the Rest API, and adds it to the
    list of procs for the run/sample. Called when a Dataset starts processing. This way a processing history
    is built up.
  - sync - Pushes `self.entity` to the Rest API
  - update_entity - Convenience method. Updates `self.entity` from kwargs and syncs.
  - change_status, start, finish - Methods for starting, finishing, resetting, etc. the dataset.
  - start_stage - Creates a stage in `analysis_driver_stages` and adds it to `self.entity`.
  - end_stage - Updates a given stage with an end time and exit status.

### notification
Subclasses the EGCG-Core `NotificationCentre`, adding pipeline-specific messaging for `start_pipeline`,
`end_pipeline`, `start_stage`, `end_stage` and `crash_report`.

### dataset_scanner
Dataset scanners handle the fetching of Datasets from the Rest API and the creation of new Datasets upon
 starting the pipeline.

- DatasetScanner - Base class for scanning for datasets.
  - scan_datasets - Uses `self._list_datasets` to list datasets and sort them by status. Returns a dict of
    lists.
  - report - Prints the results of `self.scan_datasets`
  - list_datasets - NotImplemeneted
  - get_dataset - NotImplemented
- RunScanner
  - list_datasets - Lists the contents of the config's run input dir
  - get_dataset - Returns a RunDataset based on a given name and input dir location
- SampleScanner
  - list_datasets - Lists all sample datasets in the Rest API
  - get_dataset - Returns a SampleDataset for the given name

### pipelines
This contains the various logic workflows that Analysis-Driver is capable of.

- pipeline - Takes a dataset object, decides which pipeline to use and runs it.
- demultiplexing_pipeline - Runs bcl2fastq on sequencer BCL files, runs QC on the resulting data, and outputs
  it. Also outputs extra files produced during sequencing and demultiplexing.
- var_calling_pipeline - Finds the relevant fastq files for the SampleDataset, merges them in the job folder
  and runs bam_file_production and GATK variant calling.
- bam_file_production - Merges fastqs and runs bwa alignment. Also runs fastqc and bamtools stats.
- qc_pipeline - Runs bam_file_production and data output.
- bcbio_var_calling - Merges fastqs and runs variant calling via BCBio. Can use GATK or Freebayes.

### transfer_data

- prepare_sample_data - Finds all fastq files for all run elements in a dataset object.
- create_links_from_bcbio - Uses etc/output_files.yaml to find output files to be picked up and symbolically
  links them to a 'to be outputted' directory in the jobs folder.
- output_run_data - rsyncs the data across.
- output_sample_data - rsyncs the data across, using the user's supplied sample ID (from the LIMS) as the
  output dirname.


## Howto
The Analysis Driver is run from bin/edingen_analysis_driver.py:

    python bin/edingen_analysis_driver.py <args>

Valid arguments include:
- --debug - Run with log levels set to `DEBUG`
- --report - Run `scan_datasets` only. Will not list (potentially numerous) completed/failed datasets.
- --report-all - As above, but report all datasets.
- --skip <datasets...> - Mark the specified datasets as `complete`.
- --reset <datasets...> - Mark the specified datasets with `reprocess`.
- --abort <datasets...> - Mark specified datasets as `aborted`. These will not be picked up by the dataset
  scanner.

If no args are given, one new dataset will be processed. Run recurrently through Cron to periodically scan
datasets and kick off one

To ignore datasets (or indeed any non-dataset directories) in the configured location, a `.triggerignore` file
can be written in the same place, where each line is the folder name. Any directories listed in this file will
not be picked up by the scanner.

Within any input dataset, there will need to be a `RunInfo.xml` and a `SampleSheet.csv`. These contain
information about the sequencing runs. A working directory will be created in the (configured) jobs folder
where intermediate and output data is kept.
